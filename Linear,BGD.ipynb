{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性单元 梯度下降\n",
    "\n",
    "### 线性单元的目标函数\n",
    "\n",
    "\n",
    "线性模型，常用于回归问题，而不是分类问题：$$y=h(x)=w^{T}x$$\n",
    "\n",
    ">监督学习：每个训练样本既包含输入特征**x**也包括对于输出**y**（也叫标记，Label）。\n",
    "\n",
    ">无监督学习：每个训练样本只包含输入特征**x**，不包含对于输出**y**，模型可以总结x的规律，但无法知道对应的y。\n",
    "\n",
    "对于监督学习，单个样本的误差可以用下述公式表示：$$e=\\frac{1}{2}(y-\\bar{y})^2$$\n",
    "\n",
    "那么整个模型的误差(越小越好)$$E(w)=\\sum^{n}_{i=1}e^{(i)}=\\frac{1}{2}\\sum^{n}_{i=1}(y^{(i)}-\\bar{y}^{(i)})^2$$\n",
    "\n",
    ">其中，$\\bar{y}$为预测值，$y$为实际值。即$\\bar{y}=h(x^{(i)})=w^{T}x^{(i)}$\n",
    "\n",
    "模型的训练就是寻找合适的w，使得$E(w)$最小，该过程叫优化问题。$E(w)$就是优化的目标，叫目标函数。\n",
    "\n",
    "### 梯度下降优化算法\n",
    "\n",
    "\n",
    "\n",
    "梯度下降：每次改变x的值都是向函数$y=f(x)$的梯度的相反方向修改，因此总能往最小值方向（沿梯度方向，函数值增加最快，反之减小最快。）。总结成公式即为$$x_new = x_old -\\eta\\nabla{f(x)}$$\n",
    "\n",
    "$\\eta$是学习速率（太大可能会越过极值点，太小会迭代很多次），$\\nabla$是梯度算子，$\\nabla{f(x)}$是$f(x)$的梯度。由上节的公式，梯度下降算法有可以写成：$$w_new = w_old -\\eta\\nabla{E(w)}$$\n",
    "\n",
    "经过计算$$\\nabla{E(w)}=-\\sum^{n}_{i=1}(y^{(i)}-\\bar{y}^{(i)})x^{(i)}$$\n",
    "\n",
    ">其中，**w**,**x**是**M+1**维列向量（因为包含偏置b），**y**是标量，该算法又称批梯度下降（batch gradient descent）。数据量大的时候要使用随机梯度下降算法。\n",
    "\n",
    "### $\\nabla{E(w)}$的推导\n",
    "\n",
    "$$\\nabla{E(w)}=\\frac{\\partial}{\\partial{w}}E(w)=\n",
    "\\frac{1}{2}\\frac{\\partial}{\\partial{w}}\\sum^{n}_{i=1}(y^{(i)}-\\bar{y}^{(i)})^2=\n",
    "\\frac{1}{2}\\sum^{n}_{i=1}\\frac{\\partial}{\\partial{w}}(y^{(i)2}-2\\cdot y^{(i)}\\bar{y}^{(i)}+\\bar{y}^{(i)2})$$\n",
    "\n",
    "由链式求导法则：$$\\frac{\\partial}{\\partial{w}}E(w) = \n",
    "\\frac{\\partial{E(w)}}{\\partial{\\bar{y}}}\\frac{\\partial{\\bar{y}}}{\\partial{w}}$$\n",
    "\n",
    "其中：\n",
    "$$\\frac{\\partial{E(w)}}{\\partial{\\bar{y}}}=\n",
    "\\frac{\\partial}{\\partial{\\bar{y}}}(y^{(i)2}-2\\cdot y^{(i)}\\bar{y}^{(i)}+\\bar{y}^{(i)2})=\n",
    "-2y^{(i)}+2\\bar{y}^{(i)}$$\n",
    "\n",
    "$$\\frac{\\partial{\\bar{y}}}{\\partial{w}}=\\frac{\\partial{w^{T}x}}{\\partial{w}}=x$$\n",
    "\n",
    "因此：\n",
    "$$\\nabla{E(w)}=\n",
    "\\frac{1}{2}\\sum^{n}_{i=1}\\frac{\\partial}{\\partial{w}}(y^{(i)2}-2\\cdot y^{(i)}\\bar{y}^{(i)}+\\bar{y}^{(i)2})=\n",
    "\\frac{1}{2}\\sum^{n}_{i=1}(-2y^{(i)}+2\\bar{y}^{(i)})x=\n",
    "-\\sum^{n}_{i=1}(\\bar{y}^{(i)}-y^{(i)})x$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
